---
title: User Lifecycle Management: Part 2 ‚Äì collector
cover_title: User Lifecycle Management: Part 2 ‚Äì collector
description: Crafting middleware with libpq to handle notifications in real-time and collect actionable tokens in batches.
tags: iam,c,mailroom,io
published: 2025-01-10T14:47:00
updated: 2025-01-10T14:47:00
---

> Crafting middleware with libpq to handle notifications in real-time and collect actionable tokens in batches.

[Previously in this series](/mailroom-i.html), we explored **triggers** and their role in managing a token queue atop PostgreSQL. Although we lightly incorporated **NOTIFY** statements, we haven't fully examined their power yet. In this segment, we'll bring **notification events** into focus by developing a **collector** to gather and seamlessly prepare them for downstream processing.

#### Setting Up Your Environment

Let's quickly set up the database you'll need for this part. Run the following command to create a new database:

```sh
createdb mailroom
```

Navigate to the `migrations` folder in the repository and run:

```sh
psql -d mailroom < 0_init.up.sql
```

You can also use [go-migrate](https://github.com/golang-migrate/) which I often prefer.

But that's it‚Äîyour database is ready to go!

#### Inspect the Initial State

Before we pre-fill the database with some mock data, let's check out the initial state of the `jobs` table. Run this command:

```sh
psql -d mailroom -c "SELECT * FROM jobs;"
```

You should see this:

```
 job_type | last_seq
----------+----------
 mailroom |        0
(1 row)
```

#### Start Listening for Updates

To monitor notification events, connect to the database in another terminal using `psql -d mailroom`, then type the following command:

```
LISTEN token_insert;
```

#### Populate with Sample Data

Now, let's add some dummy data to the `accounts` table. This command will insert 3 records with randomized email and login fields:

```sh
printf "%.0sINSERT INTO accounts (email, login) VALUES ('user' || md5(random()::text) || '@fake.mail', 'user' || substr(md5(random()::text), 1, 20));\n" {1..3} | \
    psql -d mailroom
```

Here's an example of what an inserted `account` record looks like:

```
-[ ACCOUNT 1 ]-------------------------------------------------------------------
id                | 1
email             | usere3213152e8cdf722466a011b1eaa3c98@fake.mail
status            | provisioned
login             | user85341405cb33cbe89a5f
created_at        | 1735709763
status_changed_at |
activated_at      |
suspended_at      |
unsuspended_at    |
```

And here's the corresponding `token` record that was automatically generated by the trigger function:

```
-[ TOKEN 1 ]---------------------------------------------------------------------
id          | 1
action      | activation
secret      | \x144d3ba23d4e60f80d3cb5cf25783539ba267af34aecd71d7cc888643c912fb7
code        | 06435
account     | 1
expires_at  | 1735710663
consumed_at |
created_at  | 1735709763
```

Don't worry if a notification hasn't appeared in the other terminal‚Äî`psql` might just need a little nudge (`;`) to display them:

```
mailroom=# LISTEN token_insert;
LISTEN
mailroom=# ;
Asynchronous notification "token_insert" received from server process with PID 5148.
Asynchronous notification "token_insert" received from server process with PID 5148.
Asynchronous notification "token_insert" received from server process with PID 5148.
```

#### Dequeue Pending Jobs

These notifications signal the collector that it's time to execute the query we formulated earlier in Part 1:

```sql
WITH token_data AS (
  SELECT
    t.account,
    t.secret,
    t.code,
    t.expires_at,
    t.id,
    t.action,
    a.email,
    a.login
  FROM
    jobs
  JOIN tokens t
    ON t.id > jobs.last_seq
    AND t.expires_at > EXTRACT(EPOCH FROM NOW())
    AND t.consumed_at IS NULL
    AND t.action IN ('activation', 'password_recovery')
  JOIN accounts a
    ON a.id = t.account
    AND (
      (t.action = 'activation' AND a.status = 'provisioned')
      OR (t.action = 'password_recovery' AND a.status = 'active')
    )
  WHERE
    jobs.job_type = 'mailroom'
  ORDER BY id ASC
  LIMIT 10
),
updated_jobs AS (
  UPDATE
    jobs
  SET
    last_seq = (SELECT MAX(id) FROM token_data)
  WHERE
    EXISTS (SELECT 1 FROM token_data)
  RETURNING last_seq
)
SELECT
  td.action,
  td.email,
  td.login,
  td.secret,
  td.code
FROM
  token_data td
```

... which accomplishes two things:

1. It retrieves tokens generated after the last seen sequence along with the corresponding user data.
2. It updates the `last_seq` value to prevent duplicate records from being selected in future executions.

In other words, it retrieves _a batch_ and advances the cursor:

```
-[ RECORD 1 ]--------------------------------------------------------------
action | activation
email  | usere3213152e8cdf722466a011b1eaa3c98@fake.mail
login  | user85341405cb33cbe89a5f
secret | \x144d3ba23d4e60f80d3cb5cf25783539ba267af34aecd71d7cc888643c912fb7
code   | 06435
-[ RECORD 2 ]--------------------------------------------------------------
action | activation
email  | user41e8b6830c76870594161150051f8215@fake.mail
login  | user2491d87beb8950b4abd7
secret | \x27100e07220b62e849e788e6554fede60c96e967c4aa62db7dc45150c51be23f
code   | 80252
-[ RECORD 3 ]--------------------------------------------------------------
action | activation
email  | user7bb11e235c85afe12076884d06910be4@fake.mail
login  | user91ab8536cb05c37ff46a
secret | \xa9763eec727835bd97b79018b308613268d9ea0db70493fd212771c9b7c3bcb2
code   | 31620
```

# Overview

The `collector` acts as a bridge between the database and the email sender. Its job is to buffer and control the flow of emails, fetching batches of payloads triggered by notifications and forwarding them to the downstream process responsible for actually sending emails.

#### Optimizing Resource Utilization

Without notifications, we would need to periodically query the database every few seconds to check for pending jobs. While this approach might be reasonable in high-traffic scenarios, it is wasteful for most websites. If token insertions are rare, continuous polling just eats up resources. By listening for database notifications instead, the `collector` can respond in near real-time and only query when there's actual work to do.

#### Staying within Cost Constraints

On the flip side, the limits imposed by our email provider‚Äîand, by extension, our budget‚Äîbecome critical constraints to consider. For example, with Amazon SES charging **$0.10 per 1,000 emails**, a monthly budget of **$100** translates to:

- **1,000,000 emails per month,**
- **33,333 emails per day,**
- **1,389 emails per hour,**
- **23 emails per minute, and**
- **0.38 emails per second.**

At this rate, the `collector` would need to buffer for approximately **27 seconds** for batches containing **10 destinations** each:

```
10 emails / 0.38 emails per second ‚âà 26.32 seconds
```

> Despite this, for high availability, we will permit short bursts capped at a rate of **50 emails per second** (within a defined daily limit), matching the [bulk API](https://docs.aws.amazon.com/ses/latest/APIReference/API_SendBulkTemplatedEmail.html) limit for maximum destinations in a single call. This rate is, of course, far above the strict constraints of a $100 budget, but such flexibility is crucial for the system to respond quickly to spikes in demand, even if it briefly exceeds cost-efficient thresholds.

## Batching Strategy

While the system can push out 50 emails in a single second (and likely far, far beyond that), doing so continuously would blow past our budget. Additionally, users are generally okay with waiting a short time for these types of emails to arrive. Personally, I wouldn't be too concerned if my password recovery email arrived within 30 seconds, though waiting a full minute might feel excessive.

### Key Variables: Batch Timeout & Batch Limit

To manage these considerations, the collector uses two main parameters:

- **Batch limit:** The maximum number of email destinations in a single batch‚Äîcapped at 50.
- **Batch timeout:** The time (in milliseconds) to wait for accumulating enough notifications to fill a batch.

#### Example

If you set:

- A batch **timeout** of 30 seconds.
- A **limit** of 10 notifications.

This means:

- If 10 notifications arrive in quick succession, the batch is triggered immediately.
- If fewer than 10 arrive over 30 seconds, the batch is triggered when the timeout ends.

In short, the `collector` focuses on batching within constraints. It passes the workload to the `sender` whenever the **batch limit** is reached or the **batch timeout** expires. The `sender` can then decide how best to stay within budget and provider limits, making sure the system remains responsive without blowing through resources.

> The `collector` processes tasks within the configured limits. However, the underlying queue mechanism is basic‚Äîit processes payloads without distinguishing between their types and lacks the ability to prioritize critical ones, such as password recovery, over less urgent emails like account activations. At present, '10 emails per second' could mean 10 emails of the same type or a mix, depending on the batch. While effective, this design leaves room for improvement, such as introducing prioritization or smarter batching strategies.

# Implementation

The `collector` source code is straightforward. Written in C and primarily using [libpq](https://www.postgresql.org/docs/current/libpq.html), it listens for notifications, counts them, executes a query, and outputs the result.

## Connecting to PostgreSQL

The query we constructed earlier is defined in the [`db.c`](https://github.com/tetsuo/mailroom/blob/master/collector/src/db.c) file, along with all other database-related functions. Here's how the connection is initialized, the `LISTEN` command is executed, and prepared statements are created. These operations are performed only once during connection:

```c
// Creates a prepared statement to be reused for efficient database queries.
static bool db_prepare_statement(PGconn *conn, const char *stmt_name, const char *query)
{
  PGresult *res = PQprepare(conn, stmt_name, query, 2, NULL);
  if (PQresultStatus(res) != PGRES_COMMAND_OK)
  {
    PQclear(res);
    return false;
  }
  PQclear(res);
  return true;
}

// Executes a LISTEN command on a specified channel to receive database
// notifications in real time.
static bool db_listen(PGconn *conn, const char *channel)
{
  char *escaped_channel = PQescapeIdentifier(conn, channel, strlen(channel));
  if (!escaped_channel)
  {
    return false;
  }

  size_t command_len = strlen("LISTEN ") + strlen(escaped_channel) + 1;
  char listen_command[command_len];
  snprintf(listen_command, command_len, "LISTEN %s", escaped_channel);
  PQfreemem(escaped_channel);

  PGresult *res = PQexec(conn, listen_command);
  if (PQresultStatus(res) != PGRES_COMMAND_OK)
  {
    PQclear(res);
    return false;
  }
  PQclear(res);

  return true;
}

// Establishes a connection to the database, listens for notifications, and
// creates prepared statements.
bool db_connect(PGconn **conn, const char *conninfo, const char *channel)
{
  *conn = PQconnectdb(conninfo);

  return PQstatus(*conn) == CONNECTION_OK &&
         db_listen(*conn, channel) &&
         db_prepare_statement(*conn, POSTGRES_HEALTHCHECK_PREPARED_STMT_NAME, "SELECT 1") &&
         db_prepare_statement(*conn, POSTGRES_DATA_PREPARED_STMT_NAME, token_data);
}
```

## Fetching Email Payloads

The `db_dequeue` function handles the heavy lifting here: it executes the query to fetch batches of payloads and writes the results directly to stdout, just like that:

```c
// Fetches and processes email payloads from the database.
static int _db_dequeue(PGconn *conn, const char *queue, int limit)
{
  static const char *params[2];
  static char limitstr[12];

  PGresult *res = NULL;
  int action_col, email_col, login_col, code_col, secret_col;
  char *action, *email, *login, *code, *secret_text;
  unsigned char *secret = NULL;
  size_t secret_len;
  int nrows;

  static char signature_buffer[SIGNATURE_MAX_INPUT_SIZE];  // Input to sign
  static unsigned char hmac_result[HMAC_RESULT_SIZE];      // HMAC output
  static unsigned char combined_buffer[CONCATENATED_SIZE]; // secret + HMAC
  static char base64_encoded[BASE64_ENCODED_SIZE];         // Base64-encoded output

  size_t hmac_len = 0;

  snprintf(limitstr, sizeof(limitstr), "%d", limit);
  params[0] = queue;
  params[1] = limitstr;

  res = PQexecPrepared(conn, POSTGRES_DATA_PREPARED_STMT_NAME, 2, params, NULL, NULL, 0);
  if (PQresultStatus(res) != PGRES_TUPLES_OK)
  {
    log_printf("ERROR: query execution failed: %s", PQerrorMessage(conn));
    PQclear(res);
    return -1;
  }

  nrows = PQntuples(res);
  if (nrows == 0)
  {
    PQclear(res);
    return 0;
  }

  action_col = PQfnumber(res, "action");
  email_col = PQfnumber(res, "email");
  login_col = PQfnumber(res, "login");
  code_col = PQfnumber(res, "code");
  secret_col = PQfnumber(res, "secret");

  if (action_col == -1 || email_col == -1 || login_col == -1 ||
      code_col == -1 || secret_col == -1)
  {
    log_printf("FATAL: missing columns in the result set");
    PQclear(res);
    return -2;
  }

  size_t signature_len;

  for (int i = 0; i < nrows; i++)
  {
    action = PQgetvalue(res, i, action_col);
    email = PQgetvalue(res, i, email_col);
    login = PQgetvalue(res, i, login_col);
    code = PQgetvalue(res, i, code_col);
    secret_text = PQgetvalue(res, i, secret_col);

    secret = PQunescapeBytea((unsigned char *)secret_text, &secret_len);
    if (!secret || secret_len != 32)
    {
      log_printf("WARN: skipping row; PQunescapeBytea failed or invalid secret length");
      continue;
    }

    if (strcmp(action, "activation") == 0)
    {
      printf("%d", 1);
    }
    else if (strcmp(action, "password_recovery") == 0)
    {
      printf("%d", 2);
    }
    else
    {
      printf("%d", 0);
    }

    printf(",%s,%s,", email, login);

    signature_len = construct_signature_data(signature_buffer, action, secret, code);

    hmac_len = HMAC_RESULT_SIZE;
    if (!hmac_sign(signature_buffer, signature_len, hmac_result, &hmac_len))
    {
      log_printf("WARN: skipping row; HMAC signing failed");
      PQfreemem(secret);
      continue;
    }

    memcpy(combined_buffer, secret, 32);
    memcpy(combined_buffer + 32, hmac_result, hmac_len);

    if (!base64_urlencode(base64_encoded, sizeof(base64_encoded), combined_buffer, 32 + hmac_len))
    {
      log_printf("WARN: skipping row; base64 encoding failed");
      PQfreemem(secret);
      continue;
    }

    printf("%s,%s", base64_encoded, code);

    PQfreemem(secret);

    if (i < nrows - 1)
    {
      printf(",");
    }
  }

  printf("\n");
  fflush(stdout);
  PQclear(res);

  return nrows;
}

// Ensure no more than the batch limit is dequeued.
int db_dequeue(PGconn *conn, const char *queue, int remaining, int max_chunk_size)
{
  int result = 0;
  int chunk_size = 0;
  int total = 0;
  while (remaining > 0)
  {
    chunk_size = remaining > max_chunk_size ? max_chunk_size : remaining;
    result = _db_dequeue(conn, queue, chunk_size);
    if (result < 0)
    {
      return result;
    }
    total += result;
    remaining -= chunk_size;
    sleep_microseconds(10000); // 10ms
  }
  return total;
}
```

The exported function processes notifications in chunks, calling `_db_dequeue` iteratively with a `chunk_size` set to the smaller of `remaining` or `max_chunk_size`. Processing continues until all notifications are handled or an error occurs and includes a **10ms** delay between database calls.

### The `secret` Sauce

The only significant transformation occurs with the `secret` value. This value is signed using HMAC-SHA256, after which it is Base64 URL-encoded. The encoded output contains a path name (e.g., `/activate` or `/recover`), the original secret (and code, in the case of recovery), and the resulting cryptographic signature.

The signing and encoding functions are implemented in [`hmac.c`](https://github.com/tetsuo/mailroom/blob/master/collector/src/hmac.c) and [`base64.c`](https://github.com/tetsuo/mailroom/blob/master/collector/src/base64.c), respectively, using standard [OpenSSL](https://github.com/openssl/openssl) functions.

For the final `secret` format, check out this function in [`db.c`](https://github.com/tetsuo/mailroom/blob/master/collector/src/db.c#L80C1-L103C2):

```c
static size_t construct_signature_data(char *output, const char *action,
                                       const unsigned char *secret, const char *code)
{
  size_t offset = 0;

  if (strcmp(action, "activation") == 0)
  {
    memcpy(output, "/activate", 9); // "/activate" is 9 bytes
    offset = 9;
    memcpy(output + offset, secret, 32);
    offset += 32;
  }
  else if (strcmp(action, "password_recovery") == 0)
  {
    memcpy(output, "/recover", 8); // "/recover" is 8 bytes
    offset = 8;
    memcpy(output + offset, secret, 32);
    offset += 32;
    memcpy(output + offset, code, 5); // code is 5 bytes
    offset += 5;
  }

  return offset; // Total length of the constructed data
}
```

To complement this, I've included a [`verifyHmac.js`](https://github.com/tetsuo/mailroom/blob/master/etc/verifyHmac.js) script in the repository that shows how to decode and verify secrets.

#### The Case for Signed Secrets

We want to avoid having every unauthenticated request to `/activate/{SECRET}` trigger a database call. Instead, the frontend server validates the token's signature to confirm its authenticity and ensure the path name matches the intended purpose on our frontend server. Only after these checks do we query the database.

> For additional security, you'll need to handle expired tokens. One approach is to include the `expires_at` value, allowing the validity period to be checked. However, since tokens remain valid for at least 15 minutes, this alone may not be sufficient. A more robust solution is to **cache consumed tokens** until they naturally expire, preventing reuse during their validity period.

## Putting It All Together

Now that we've covered how to connect to the database, initiate the listener, and execute queries, it's time to bring everything together.

#### Environment Variables

The [`main.c`](https://github.com/tetsuo/mailroom/blob/master/collector/src/main.c) file begins by defining the necessary environment variables. You're already familiar with the key ones, such as `MAILROOM_BATCH_TIMEOUT` and `MAILROOM_BATCH_LIMIT`. Another critical variable is `MAILROOM_SECRET_KEY`, which should be a 64-character hex string (equivalent to a 32-byte securely random value) used as the signing key. For a complete list of variables, refer to the [README](https://github.com/tetsuo/mailroom/blob/master/README.md) file in the repository.

### The Loop

Finally, the heart of the operation is the main loop.

Here's a high-level overview in pseudo-code:

```
// üåü Main processing loop
WHILE the application is running üîÑ
    // üîå Handle reconnection if needed
    IF the connection is not ready ‚ùå THEN
        reconnect to the database üîÑ
        initialize the connection ‚úÖ
        reset counters üî¢
        CONTINUE to the next iteration ‚è©
    END IF

    // üì¶ Process ready batches
    IF ready for processing ‚úÖ THEN
        dequeue and process a batch of items üì§
        reset state for the next cycle üîÅ
        CONTINUE to the next iteration ‚è©
    END IF

    // üõéÔ∏è Handle pending notifications
    process all incoming notifications üì•
    IF notifications exceed the batch limit üö® THEN
        mark ready for processing ‚úÖ
        CONTINUE to the next iteration ‚è©
    END IF

    // ‚è±Ô∏è Wait for new events or timeout
    wait for activity on the connection üì° or timeout ‚åõ
    IF interrupted by a signal üö® THEN
        handle the signal (e.g., shutdown) ‚ùå
        CONTINUE to the next iteration ‚è©
    ELSE IF timeout occurs ‚è≥ THEN
        IF notifications exist üìã THEN
            mark ready for processing ‚úÖ
            CONTINUE to the next iteration ‚è©
        END IF
        perform periodic health checks ü©∫
    END IF

    // üõ†Ô∏è Consume available data
    consume data from the connection üì∂
    prepare for the next cycle üîÅ
END WHILE
```

And here's the actual implementation:

```c
int result;

PGconn *conn = NULL;

fd_set active_fds, read_fds;
int sock;

struct timeval tv;
int seen = 0;

PGnotify *notify = NULL;
int rc = 0;

long start = get_current_time_ms();
long now, elapsed, remaining_ms;

long last_healthcheck = start;

int ready = -1;

while (running)
{
  if (ready < 0)
  {
    if (conn)
    {
      PQfinish(conn);
    }

    if (!db_connect(&conn, conninfo, channel_name))
    {
      log_printf("ERROR: connection failed: %s", PQerrorMessage(conn));
      return exit_code(conn, EXIT_FAILURE);
    }

    log_printf("connected");

    while (running && (result = db_dequeue(conn, queue_name, batch_limit, batch_limit)) == batch_limit)
      ;

    if (result < 0)
    {
      return exit_code(conn, EXIT_FAILURE);
    }

    FD_ZERO(&active_fds);
    sock = PQsocket(conn);
    FD_SET(sock, &active_fds);

    seen = 0;
    ready = 0;
    last_healthcheck = get_current_time_ms();

    continue;
  }
  else if (ready > 0)
  {
    result = db_dequeue(conn, queue_name, seen, batch_limit);
    if (result == -2)
    {
      return exit_code(conn, EXIT_FAILURE);
    }
    else if (result == -1)
    {
      log_printf("WARN: forcing reconnect...");
      ready = -1;
      continue;
    }
    else if (result != seen)
    {
      log_printf("WARN: expected %d items to be processed, got %d", seen, result);
    }

    seen = 0;
    ready = 0;
    last_healthcheck = get_current_time_ms();
  }

  // Process any pending notifications before select()
  while (running && (notify = PQnotifies(conn)) != NULL)
  {
    PQfreemem(notify);
    if (seen == 0)
    {
      log_printf("NOTIFY called; waking up");
      start = get_current_time_ms(); // Received first notification; reset timer
    }
    seen++;
    PQconsumeInput(conn);
  }

  if (seen >= batch_limit)
  {
    log_printf("processing %d rows... (max reached)", seen);

    ready = 1;
    continue; // Skip select() and process immediately
  }

  now = get_current_time_ms();
  elapsed = now - start;
  remaining_ms = timeout_ms - elapsed;

  if (remaining_ms < 0)
  {
    remaining_ms = 0;
  }

  tv.tv_sec = remaining_ms / 1000;
  tv.tv_usec = (remaining_ms % 1000) * 1000;

  read_fds = active_fds;

  rc = select(sock + 1, &read_fds, NULL, NULL, &tv);

  if (rc < 0)
  {
    if (errno == EINTR)
    {
      if (!running)
      {
        break;
      }
      log_printf("WARN: select interrupted by signal");
      continue;
    }
    log_printf("ERROR: select failed: %s (socket=%d)", strerror(errno), sock);
    break;
  }
  else if (rc == 0)
  {                                // Timeout occurred;
    start = get_current_time_ms(); // Reset the timer

    if (seen > 0)
    {
      log_printf("processing %d rows... (timeout)", seen);

      ready = 1;
      continue;
    }

    if ((sock = PQsocket(conn)) < 0)
    {
      log_printf("WARN: socket closed; %s", PQerrorMessage(conn));
      ready = -1;
      continue;
    }

    if (now - last_healthcheck >= healthcheck_ms)
    {
      if (!db_healthcheck(conn))
      {
        ready = -1;
        continue;
      }
      else
      {
        last_healthcheck = start;
      }
    }
  }

  if (!FD_ISSET(sock, &read_fds))
  {
    continue;
  }

  do
  {
    if (!PQconsumeInput(conn))
    {
      log_printf("WARN: error consuming input: %s", PQerrorMessage(conn));
      if (PQstatus(conn) != CONNECTION_OK)
      {
        ready = -1;
        break;
      }
    }
  } while (running && PQisBusy(conn));
}
```

#### **The `select()` Call**

The [`select()`](https://man7.org/linux/man-pages/man2/select.2.html) system call is central to how the program operates. It's a UNIX mechanism that monitors file descriptors (like sockets) to determine if they're ready for I/O operations (e.g., reading or writing).

In this code, `select()` is used to:

- Wait for notifications from PostgreSQL via the socket associated with the database connection.
- Set a timeout to trigger periodic batch processing, even during inactivity.

When data becomes available on the socket, `select()` returns, allowing the program to handle notifications immediately. If the timeout elapses without activity, the program processes the current batch instead.

#### **`PQconsumeInput` and `PQnotifies`**

The `collector` uses [libpq's async API](https://www.postgresql.org/docs/current/libpq-async.html) to handle PostgreSQL notifications:

- **`PQconsumeInput`**: Reads data from the socket into libpq's internal buffer, capturing incoming notifications.
- **`PQnotifies`**: Retrieves pending notifications from the buffer. This function requires a prior call to `PQconsumeInput`; otherwise, no data will be available to process.

# Compile & Run

To compile `collector`, verify that `openssl@3` and `libpq@5` are installed on your system, then use the provided `Makefile`.

#### Makefile Targets

  - `release`: Compiles an optimized binary for production use. Default target.
  - `debug`: Compiles a binary with debug symbols for development.
  - `clean`: Removes build artifacts.

---

### Run the Collector

Use the following command to build and run the `collector` with example configuration variables:

```sh
make && \
  MAILROOM_BATCH_LIMIT=3 \
  MAILROOM_BATCH_TIMEOUT=5000 \
  MAILROOM_DATABASE_URL="dbname=mailroom" \
  MAILROOM_SECRET_KEY="cafebabecafebabecafebabecafebabecafebabecafebabecafebabecafebabe" \
  ./collector
```

Once configured and started, it will log its activity:

```
2024/04/20 13:37:00 [PG] configured; channel=token_insert queue=mailroom limit=3 timeout=5000ms healthcheck-interval=270000ms
2024/04/20 13:37:00 [PG] connecting to host=/tmp port=5432 dbname=mailroom user=ogu sslmode=disable
2024/04/20 13:37:00 [PG] connected
```

---

### Insert Accounts and Observe Batching

In another terminal, insert 5 accounts into the database:

```sh
printf "%.0sINSERT INTO accounts (email, login) VALUES ('user' || md5(random()::text) || '@fake.mail', 'user' || substr(md5(random()::text), 1, 20));\n" {1..5} | \
    psql -d mailroom
```

The `collector` should immediately process and output the first batch containing 3 items. After a 5-second delay (as defined by the `MAILROOM_BATCH_TIMEOUT`), it will process the remaining 2 items in a second batch:

```
2024/04/20 13:37:00 [PG] NOTIFY called; waking up
2024/04/20 13:37:00 [PG] processing 3 rows... (max reached)
1,userb183abb7a25d04027061e6b8d8d8e7fa@fake.mail,userb0bf075b82b892f53d97,gVRNesi-opSvs3ntPfr9DzSn_JwbOD04VVIurQSCOFzzd3BOM3WBDL3SOtDjMxKLd6csSn8_p9hemXHIUxIjPg,78092,1,user43b01ba9686c886473e526429dd2c672@fake.mail,userf420078dba4fd5a91de2,--DTy5LsbDeLP_AweXIPSjL3_avQMT5cH_bRxPy1uxQLVhXKaw7Oxd7NYkcJ6MZmnnqWqTcBPHA5z7bqunXEAA,25778,1,user46f81dfd34b91a1904ac4524193575aa@fake.mail,user6d91baab56d2823b326d,ryooWewe3OTxIGF1Gjl5Vvl8BsXoqWVbCAt1t6J--_KX1SM4DbyCes4yn75OWVe60G4MMZdv4byRh1wy-Clvxw,78202
2024/04/20 13:37:00 [PG] NOTIFY called; waking up
2024/04/20 13:37:05 [PG] processing 2 rows... (timeout)
1,user12d2722e1c07b0a531ea69ae125d4697@fake.mail,user853ae29eefc5d44a6bc6,4pmew2o2EOAZBDHWvJBcixJftpRCb8uyXZhzN12EOcrLBmzc4ic9avwd9dla09pIiKIoqW5iIwMfoXLEM3_LGw,38806,1,user9497d0e033019fcf3198eecb053ba40e@fake.mail,userfcde338dba96cc419613,ANLMa-1y37VLCDqK0wnfEFhUVzHsWpaNGV2ttI8m3o6_lbbYOKmp3hP7Q8H8ZQRNMPAj4xsSqC26nesfVZLgzQ,89897
```

---

### Testing Reconnect Behavior

To test the reconnect mechanism, open another terminal and connect to the database: `psql -d mailroom`

Then, execute the following command to terminate all connections to the `mailroom` database except your current one. This will simulate a connection drop.

```sql
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE
  datname = 'mailroom'
  AND pid <> pg_backend_pid();
```

After the connection is terminated, `select()` will wake up, resulting in `PQconsumeInput()` failing with an error. This failure will initiate a reconnect attempt:

```
2024/04/20 13:37:42 [PG] WARN: error consuming input: server closed the connection unexpectedly
        This probably means the server terminated abnormally
        before or while processing the request.

2024/04/20 13:37:42 [PG] connecting to host=/tmp port=5432 dbname=mailroom user=ogu sslmode=disable
2024/04/20 13:37:42 [PG] connected
```

# Next

In Part 3, we'll implement the sender component to process collected events and manage batch email delivery efficiently.
